{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Fine-tuning the Model\n",
    "\n",
    "In [Step 2 - Choosing a Model](Step 2 - Choosing a Model.ipynb), we settled on using a pre-trained model called **InceptionV3**.\n",
    "\n",
    "In this notebook, we take this model and fine-tune some parameters to see which combination achieves the best results. Like before, we will use the *F1 Score* metric to determine which model performs best.\n",
    "![title](notebook_images/f1_formula.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "from image_preprocessor import ImagePreprocessor\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image as Image\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = os.path.join('..', 'application', 'images')\n",
    "originals_dir = os.path.join(root_dir, \"original\")\n",
    "training_dir = os.path.join(root_dir, \"train\")\n",
    "test_dir = os.path.join(root_dir, \"test\")\n",
    "val_dir = os.path.join(root_dir, \"validation\")\n",
    "\n",
    "target_imagesize = (256, 256)\n",
    "\n",
    "clear_existing_data = True # if true, data in training, test and validation directories will be deleted before splitting the data in the originals directory\n",
    "augment_data = True # whether images should be augmented during preprocessing\n",
    "augmentations = 20 # how many augmentations to make for each original image\n",
    "\n",
    "random_seed = 7\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "saved_models_dir = os.path.join('..', 'application', 'saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 image categories\n",
      "465 total images\n",
      "\n",
      "279 training images\n",
      "93 validation images\n",
      "93 test images\n",
      "\n",
      "Categories:\n",
      "  - animal\n",
      "  - city_scape\n",
      "  - food\n",
      "  - group\n",
      "  - landscape\n",
      "  - me\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ImagePreprocessor()\n",
    "preprocessor.root_dir = root_dir\n",
    "preprocessor.originals_dir = originals_dir\n",
    "preprocessor.training_dir = training_dir\n",
    "preprocessor.test_dir = test_dir\n",
    "preprocessor.val_dir = val_dir\n",
    "preprocessor.random_seed = random_seed\n",
    "preprocessor.target_imagesize = target_imagesize\n",
    "preprocessor.clear_existing_data = clear_existing_data\n",
    "\n",
    "preprocessor.initialize()\n",
    "categories = preprocessor.categories\n",
    "training_count = preprocessor.training_count\n",
    "validation_count = preprocessor.validation_count\n",
    "test_count = preprocessor.test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 279 images belonging to 6 classes.\n",
      "Found 93 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "img_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255,\n",
    "    fill_mode='reflect')\n",
    "\n",
    "train_generator = img_datagen.flow_from_directory(training_dir,\n",
    "                                                   target_size=target_imagesize,\n",
    "                                                   batch_size=augmentations,\n",
    "                                                   shuffle=True,\n",
    "                                                   seed=random_seed)\n",
    "\n",
    "validation_generator = img_datagen.flow_from_directory(val_dir,\n",
    "                                                   target_size=target_imagesize,\n",
    "                                                   batch_size=augmentations,\n",
    "                                                   shuffle=True,\n",
    "                                                   seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_category(img_path, pred_model):\n",
    "    img_tensor = preprocessor.file_to_tensor(img_path)\n",
    "    h = pred_model.predict(img_tensor)\n",
    "    return categories[np.argmax(h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_score_cal(model=None):\n",
    "    test_images = np.array(glob(os.path.join(test_dir, \"*\", \"*\")))\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for img in test_images:\n",
    "        y_true.append(categories.index(re.split(r'[\\\\/]',img)[-2]))\n",
    "        pred = predict_category(img, model)\n",
    "        y_pred.append(categories.index(pred))\n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "modelnames = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Layer Sizes\n",
    "\n",
    "In the section below, we will train the model with different sizes of the hidden layer:\n",
    "- 16 nodes\n",
    "- 32 nodes\n",
    "- 64 nodes\n",
    "- 128 nodes\n",
    "- 256 nodes\n",
    "- 512 nodes\n",
    "- 1024 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inceptionv3_16nodes\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 1s - loss: 1.0129 - acc: 0.6368"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donke\\Anaconda2\\envs\\cnn_image_classification\\lib\\site-packages\\PIL\\Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 0.65292, saving model to saved_models\\inceptionv3_16nodes.hdf5\n",
      "93/93 [==============================] - 136s - loss: 1.0064 - acc: 0.6386 - val_loss: 0.6529 - val_acc: 0.7976\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8582Epoch 00001: val_loss improved from 0.65292 to 0.56773, saving model to saved_models\\inceptionv3_16nodes.hdf5\n",
      "93/93 [==============================] - 121s - loss: 0.4091 - acc: 0.8581 - val_loss: 0.5677 - val_acc: 0.8183\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8876Epoch 00002: val_loss improved from 0.56773 to 0.48707, saving model to saved_models\\inceptionv3_16nodes.hdf5\n",
      "93/93 [==============================] - 116s - loss: 0.3489 - acc: 0.8888 - val_loss: 0.4871 - val_acc: 0.8406\n",
      "inceptionv3_32nodes\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 1.0051 - acc: 0.6343Epoch 00000: val_loss improved from inf to 0.65020, saving model to saved_models\\inceptionv3_32nodes.hdf5\n",
      "93/93 [==============================] - 136s - loss: 0.9971 - acc: 0.6372 - val_loss: 0.6502 - val_acc: 0.7993\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8692Epoch 00001: val_loss did not improve\n",
      "93/93 [==============================] - 118s - loss: 0.3874 - acc: 0.8695 - val_loss: 0.6600 - val_acc: 0.7803\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9050Epoch 00002: val_loss improved from 0.65020 to 0.54453, saving model to saved_models\\inceptionv3_32nodes.hdf5\n",
      "93/93 [==============================] - 117s - loss: 0.2846 - acc: 0.9060 - val_loss: 0.5445 - val_acc: 0.8374\n",
      "inceptionv3_64nodes\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 1.0075 - acc: 0.6561Epoch 00000: val_loss improved from inf to 0.65996, saving model to saved_models\\inceptionv3_64nodes.hdf5\n",
      "93/93 [==============================] - 148s - loss: 1.0036 - acc: 0.6566 - val_loss: 0.6600 - val_acc: 0.7820\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 1s - loss: 0.3952 - acc: 0.8689Epoch 00001: val_loss improved from 0.65996 to 0.54277, saving model to saved_models\\inceptionv3_64nodes.hdf5\n",
      "93/93 [==============================] - 140s - loss: 0.3935 - acc: 0.8698 - val_loss: 0.5428 - val_acc: 0.8336\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9039Epoch 00002: val_loss did not improve\n",
      "93/93 [==============================] - 130s - loss: 0.2864 - acc: 0.9033 - val_loss: 0.5945 - val_acc: 0.8080\n",
      "inceptionv3_128nodes\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 1s - loss: 0.9863 - acc: 0.6566Epoch 00000: val_loss improved from inf to 0.66206, saving model to saved_models\\inceptionv3_128nodes.hdf5\n",
      "93/93 [==============================] - 162s - loss: 0.9795 - acc: 0.6597 - val_loss: 0.6621 - val_acc: 0.7820\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8674Epoch 00001: val_loss improved from 0.66206 to 0.57992, saving model to saved_models\\inceptionv3_128nodes.hdf5\n",
      "93/93 [==============================] - 129s - loss: 0.3895 - acc: 0.8677 - val_loss: 0.5799 - val_acc: 0.8097\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.8969Epoch 00002: val_loss improved from 0.57992 to 0.47004, saving model to saved_models\\inceptionv3_128nodes.hdf5\n",
      "93/93 [==============================] - 129s - loss: 0.2895 - acc: 0.8969 - val_loss: 0.4700 - val_acc: 0.8408\n",
      "inceptionv3_256nodes\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 1s - loss: 0.9460 - acc: 0.6802Epoch 00000: val_loss improved from inf to 0.55347, saving model to saved_models\\inceptionv3_256nodes.hdf5\n",
      "93/93 [==============================] - 164s - loss: 0.9412 - acc: 0.6810 - val_loss: 0.5535 - val_acc: 0.8144\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8838Epoch 00001: val_loss did not improve\n",
      "93/93 [==============================] - 131s - loss: 0.3877 - acc: 0.8823 - val_loss: 0.5898 - val_acc: 0.8166\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9013Epoch 00002: val_loss improved from 0.55347 to 0.53221, saving model to saved_models\\inceptionv3_256nodes.hdf5\n",
      "93/93 [==============================] - 129s - loss: 0.3032 - acc: 0.9002 - val_loss: 0.5322 - val_acc: 0.8097\n",
      "inceptionv3_512nodes\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 1s - loss: 0.8724 - acc: 0.7037Epoch 00000: val_loss improved from inf to 0.53871, saving model to saved_models\\inceptionv3_512nodes.hdf5\n",
      "93/93 [==============================] - 175s - loss: 0.8676 - acc: 0.7058 - val_loss: 0.5387 - val_acc: 0.8235\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8689Epoch 00001: val_loss improved from 0.53871 to 0.53610, saving model to saved_models\\inceptionv3_512nodes.hdf5\n",
      "93/93 [==============================] - 127s - loss: 0.3906 - acc: 0.8681 - val_loss: 0.5361 - val_acc: 0.8080\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.8991Epoch 00002: val_loss improved from 0.53610 to 0.50894, saving model to saved_models\\inceptionv3_512nodes.hdf5\n",
      "93/93 [==============================] - 129s - loss: 0.2979 - acc: 0.8985 - val_loss: 0.5089 - val_acc: 0.8109\n",
      "inceptionv3_1024nodes\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 1s - loss: 0.8896 - acc: 0.6840Epoch 00000: val_loss improved from inf to 0.56750, saving model to saved_models\\inceptionv3_1024nodes.hdf5\n",
      "93/93 [==============================] - 187s - loss: 0.8871 - acc: 0.6847 - val_loss: 0.5675 - val_acc: 0.8270\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8795Epoch 00001: val_loss improved from 0.56750 to 0.53467, saving model to saved_models\\inceptionv3_1024nodes.hdf5\n",
      "93/93 [==============================] - 129s - loss: 0.3703 - acc: 0.8808 - val_loss: 0.5347 - val_acc: 0.8028\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.8990Epoch 00002: val_loss did not improve\n",
      "93/93 [==============================] - 130s - loss: 0.2825 - acc: 0.8995 - val_loss: 0.5938 - val_acc: 0.8253\n"
     ]
    }
   ],
   "source": [
    "for nodes in [16, 32, 64, 128, 256, 512, 1024]:\n",
    "    modelname = 'inceptionv3_' + str(nodes) + 'nodes'\n",
    "    print(modelname)\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(saved_models_dir, modelname + '.hdf5'), \n",
    "                           verbose=1, save_best_only=True)\n",
    "    \n",
    "    model = InceptionV3(include_top=False, weights = 'imagenet', input_shape = (target_imagesize[0], target_imagesize[1], 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # custom Layers \n",
    "    cus_layers = model.output\n",
    "    cus_layers = Flatten()(cus_layers)\n",
    "    \n",
    "    cus_layers = Dense(nodes, activation=\"relu\")(cus_layers)\n",
    "    predictions = Dense(len(categories), activation=\"softmax\")(cus_layers)\n",
    "    \n",
    "    model_final = Model(inputs = model.input, outputs = predictions)\n",
    "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])\n",
    "    # train the model\n",
    "    model_final.fit_generator(train_generator,\n",
    "                             steps_per_epoch=training_count // epochs, \n",
    "                              epochs=epochs,\n",
    "                             validation_data = validation_generator,\n",
    "                             validation_steps=validation_count // epochs,\n",
    "                             callbacks=[checkpointer],\n",
    "                             verbose=1)\n",
    "    \n",
    "    model_final.load_weights(filepath=os.path.join(saved_models_dir, modelname + '.hdf5'))\n",
    "    \n",
    "    models.append(model_final)\n",
    "    modelnames.append(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring the models\n",
    "\n",
    "Now that all variations of the model have been trained, we can calculate their respective F1 scores to see if any of them stand out in terms of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inceptionv3_16nodes 0.808412667198\n",
      "inceptionv3_32nodes 0.801847841651\n",
      "inceptionv3_64nodes 0.816557286497\n",
      "inceptionv3_128nodes 0.818479896273\n",
      "inceptionv3_256nodes 0.858061936774\n",
      "inceptionv3_512nodes 0.832240186162\n",
      "inceptionv3_1024nodes 0.826741946941\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    f1score = f1_score_cal(model)\n",
    "    f1_scores.append(f1score)\n",
    "    print(modelnames[i], f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x229ac6314a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(7)\n",
    "\n",
    "rects1 = ax.bar(index, f1_scores)\n",
    "\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Scores per model')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([16, 32, 64, 128, 256, 512, 1024])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The size of the hidden layer seems to be arbitrary as their results don't vary much at all. All variations took around the same time to train (between 5 and 10 minutes), so we'll just choose the layer size that gave the best result in terms of F1 score; **256 nodes** in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Values\n",
    "\n",
    "We want our model to be able to look at an image and predict what category the image should be in. We don't want our model to just memorize the images it has seen in the training phase.\n",
    "\n",
    "When a model starts memorizing images rather than learning generic features, it starts to overfit. It will have excellent results on images in the training set, but it might not be able to categorize images it hasn't seen before.\n",
    "\n",
    "One way to combat this, is by adding a **dropout layer**. With a dropout layer, we take a percentage of random nodes for each step and deactivate them. This way, the model is forced to 'forget' some of the information it learned.\n",
    "\n",
    "![title](notebook_images/nodropout_vs_dropout.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inceptionv3_256nodes_0.1dropout\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.9991 - acc: 0.6591"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donke\\Anaconda2\\envs\\cnn_image_classification\\lib\\site-packages\\PIL\\Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 0.65562, saving model to saved_models\\inceptionv3_256nodes_0.1dropout.hdf5\n",
      "93/93 [==============================] - 137s - loss: 0.9955 - acc: 0.6612 - val_loss: 0.6556 - val_acc: 0.8149\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8779Epoch 00001: val_loss improved from 0.65562 to 0.54802, saving model to saved_models\\inceptionv3_256nodes_0.1dropout.hdf5\n",
      "93/93 [==============================] - 118s - loss: 0.3912 - acc: 0.8786 - val_loss: 0.5480 - val_acc: 0.8304\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8837Epoch 00002: val_loss improved from 0.54802 to 0.51973, saving model to saved_models\\inceptionv3_256nodes_0.1dropout.hdf5\n",
      "93/93 [==============================] - 117s - loss: 0.3450 - acc: 0.8844 - val_loss: 0.5197 - val_acc: 0.8284\n",
      "inceptionv3_256nodes_0.2dropout\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 1s - loss: 1.0132 - acc: 0.6448Epoch 00000: val_loss improved from inf to 0.63271, saving model to saved_models\\inceptionv3_256nodes_0.2dropout.hdf5\n",
      "93/93 [==============================] - 150s - loss: 1.0048 - acc: 0.6481 - val_loss: 0.6327 - val_acc: 0.7924\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.8511Epoch 00001: val_loss did not improve\n",
      "93/93 [==============================] - 119s - loss: 0.4392 - acc: 0.8500 - val_loss: 0.6360 - val_acc: 0.8114\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8766Epoch 00002: val_loss improved from 0.63271 to 0.54818, saving model to saved_models\\inceptionv3_256nodes_0.2dropout.hdf5\n",
      "93/93 [==============================] - 118s - loss: 0.3397 - acc: 0.8769 - val_loss: 0.5482 - val_acc: 0.8218\n",
      "inceptionv3_256nodes_0.4dropout\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 1s - loss: 1.1490 - acc: 0.5933Epoch 00000: val_loss improved from inf to 0.69401, saving model to saved_models\\inceptionv3_256nodes_0.4dropout.hdf5\n",
      "93/93 [==============================] - 172s - loss: 1.1463 - acc: 0.5945 - val_loss: 0.6940 - val_acc: 0.7803\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.8209Epoch 00001: val_loss improved from 0.69401 to 0.54079, saving model to saved_models\\inceptionv3_256nodes_0.4dropout.hdf5\n",
      "93/93 [==============================] - 126s - loss: 0.5304 - acc: 0.8217 - val_loss: 0.5408 - val_acc: 0.8266\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8624Epoch 00002: val_loss did not improve\n",
      "93/93 [==============================] - 125s - loss: 0.4109 - acc: 0.8612 - val_loss: 0.5476 - val_acc: 0.8097\n",
      "inceptionv3_256nodes_0.5dropout\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 1s - loss: 1.2086 - acc: 0.5638Epoch 00000: val_loss improved from inf to 0.56212, saving model to saved_models\\inceptionv3_256nodes_0.5dropout.hdf5\n",
      "93/93 [==============================] - 186s - loss: 1.2031 - acc: 0.5664 - val_loss: 0.5621 - val_acc: 0.8426\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.5756 - acc: 0.8025Epoch 00001: val_loss improved from 0.56212 to 0.50118, saving model to saved_models\\inceptionv3_256nodes_0.5dropout.hdf5\n",
      "93/93 [==============================] - 127s - loss: 0.5782 - acc: 0.8020 - val_loss: 0.5012 - val_acc: 0.8235\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8392Epoch 00002: val_loss improved from 0.50118 to 0.46585, saving model to saved_models\\inceptionv3_256nodes_0.5dropout.hdf5\n",
      "93/93 [==============================] - 127s - loss: 0.4506 - acc: 0.8388 - val_loss: 0.4659 - val_acc: 0.8374\n",
      "inceptionv3_256nodes_0.8dropout\n",
      "Epoch 1/3\n",
      "92/93 [============================>.] - ETA: 1s - loss: 1.7140 - acc: 0.3927Epoch 00000: val_loss improved from inf to 0.95171, saving model to saved_models\\inceptionv3_256nodes_0.8dropout.hdf5\n",
      "93/93 [==============================] - 202s - loss: 1.7103 - acc: 0.3950 - val_loss: 0.9517 - val_acc: 0.7426\n",
      "Epoch 2/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 1.0895 - acc: 0.6047Epoch 00001: val_loss improved from 0.95171 to 0.71627, saving model to saved_models\\inceptionv3_256nodes_0.8dropout.hdf5\n",
      "93/93 [==============================] - 129s - loss: 1.0887 - acc: 0.6046 - val_loss: 0.7163 - val_acc: 0.7751\n",
      "Epoch 3/3\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.8640 - acc: 0.6981Epoch 00002: val_loss improved from 0.71627 to 0.55371, saving model to saved_models\\inceptionv3_256nodes_0.8dropout.hdf5\n",
      "93/93 [==============================] - 128s - loss: 0.8654 - acc: 0.6976 - val_loss: 0.5537 - val_acc: 0.8287\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "modelnames = []\n",
    "f1_scores = []\n",
    "for d in [0.1, 0.2, 0.4, 0.5, 0.8]:\n",
    "    model = load_model(os.path.join('saved_models', 'inceptionv3_256nodes.hdf5'))\n",
    "    modelname = 'inceptionv3_256nodes_' + str(d) + 'dropout'\n",
    "    print(modelname)\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(saved_models_dir, modelname + '.hdf5'), \n",
    "                           verbose=1, save_best_only=True)\n",
    "    \n",
    "    model = InceptionV3(include_top=False, weights = 'imagenet', input_shape = (target_imagesize[0], target_imagesize[1], 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # custom Layers \n",
    "    cus_layers = model.output\n",
    "    cus_layers = Flatten()(cus_layers)\n",
    "    \n",
    "    cus_layers = Dense(256, activation=\"relu\")(cus_layers)\n",
    "    cus_layers = Dropout(d)(cus_layers)\n",
    "    predictions = Dense(len(categories), activation=\"softmax\")(cus_layers)\n",
    "    \n",
    "    model_final = Model(inputs = model.input, outputs = predictions)\n",
    "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])\n",
    "    # train the model\n",
    "    model_final.fit_generator(train_generator,\n",
    "                             steps_per_epoch=training_count // epochs, \n",
    "                              epochs=epochs,\n",
    "                             validation_data = validation_generator,\n",
    "                             validation_steps=validation_count // epochs,\n",
    "                             callbacks=[checkpointer],\n",
    "                             verbose=1)\n",
    "    \n",
    "    model_final.load_weights(filepath=os.path.join(saved_models_dir, modelname + '.hdf5'))\n",
    "    \n",
    "    models.append(model_final)\n",
    "    modelnames.append(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring the models\n",
    "\n",
    "Like before, we will calculate the F1 scores on the new set of models to determine which one performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inceptionv3_256nodes_0.1dropout 0.826881712703\n",
      "inceptionv3_256nodes_0.2dropout 0.830371429256\n",
      "inceptionv3_256nodes_0.4dropout 0.844036058827\n",
      "inceptionv3_256nodes_0.5dropout 0.831721006114\n",
      "inceptionv3_256nodes_0.8dropout 0.850815913088\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    f1score = f1_score_cal(model)\n",
    "    f1_scores.append(f1score)\n",
    "    print(modelnames[i], f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAGCpJREFUeJzt3X+UXGd93/H3BxnF/DAmREsKkiwJKhIEBRzWNiWUH8W0Mj8kDhiQWicxMSjQCtPaoRUJcX0caAJpQ4NRW4vgmJ8WxhRHgKgSfvg0AZtqAdsgu2qFYtCiFNbGQDBgWfa3f8wVHdazq7HQ1d7Rvl/nzDn3ufeZZ777eDUfP3fu3klVIUlS1zxgrguQJGkQA0qS1EkGlCSpkwwoSVInGVCSpE4yoCRJnWRASTqqklyR5M1D9r01yZlt16TRZECp05o3sB8l+UHf49HNsS1Jdie5N8m5hxlnSZKPJLktyfeSfOVwz5E0twwojYIXVdVD+x77m/03Av8C+NIQY7wP2AcsA34B+HXgW0ezyCQnHM3xjob0+O9cI8lfXI2sqtpcVZ8GfjxE99OAK6rqzqo6WFVfrqpPHjqY5BlJPp/ku0n2HVpdJTk5yXuTTCX5epI3HXrDT3Juks8leXuS7wAXN/t/M8ktSe5IsiPJsmZ/mr7fblZxNyV54qBik1yb5A+S/M+m758neUTf8af11XtjkmdPe+5bknwO+CHwmAHj35rkDU0NdyZ5d5JfTPLJJH+X5FNJfr6v/5oku5rXuzbJ4/uOnZrkS83zPgScOO21Xpjkhua5n0/ypCH+e0kGlOaN64HNSdYlOaX/QNP+JHApMAY8BbihOXwpcDK9N/ln0Vt5vbLv6WcAe4FHAm9J8mLgd4CXNGP9FXBl0/efAM8EHgc8HHgFcPssNf868JvAo4GDwDuaehcDnwDeDDwC+G3gI0nG+p77a8AG4CTg6zOM/1LgeU09L2rm4HeARfTeG85vXu9xzc/wr5qfaTvwsSQLkywErqG3Qn0E8OFmXJrn/gpwOfBb9FaulwHbkvzcLD+3BBhQGg3XNP/3/d0k1xzhGC+jFxa/B/xN83/0pzXH/jnwqaq6sqrurqrbq+qGJAvohcgbq+rvqupW4D/Se/M/ZH9VXdqsyn5E7434D6rqlqo6CPx74CnNKupueoHxy0CaPn87S83vq6qvVtWdTd0vb2o6B9heVdur6t6q+ktgAnh+33OvqKpdTV13zzD+pVX1rar6ZjM3X2hWlncBHwVObfq9AvhEVf1lM9Z/AB4EPB14GvBA4D81c3c1sLPvNV4NXFZVX6iqe6rqPcBdzfOkWRlQGgUvrqqHN48XH8kAVXVHVW2qqicAv0hvhXRNkgBLga8NeNoiYCE/vQL5OrC4r71v2nOWAX9yKFCB7wABFlfVZ4B3ApuBbzUXeTxslrL7x/46vSBY1LzGy/pC+7vAM4BHzVLXIP2fwf1oQPuhzfaj6ZuDqrq3GX9xc+yb9dN3ne6fr2XAhdNqXdo8T5qVAaV5p6puo7cKeDS901L7gMcO6HobvVXPsr59pwDf7B9u2nP2Ab/VF6gPr6oHVdXnm9d+R1U9FXgCvVNrb5il1KXTXvfupqZ99FZX/a/xkKr6w1nq+lnsp28O+kL9m8DfAoubff21HrIPeMu0Wh9cVVciHYYBpZHVfAZyIr0VygOTnDjTFWtJ3prkiUlOSHIS8FpgT1XdDnwAODPJy5vjv5DkKVV1D3AVvc+WTmpO010AvH+Wsv4r8MYkT2he9+QkL2u2T0tyRpIHAnfSu7jjnlnGOifJqiQPBi4Brm5qej/woiT/NMmC5ud+dpIlw87d/XQV8IIkz21qv5DeabrPA9fR+3zs/GbuXgKc3vfcdwGvaX7uJHlIkhc0/w2kWRlQGmV/Qe9U1NOBLc32M2fo+2B6n6t8l95FDcuANQBV9Q16n99cSO+U3A3Ak5vnvY5emOwF/hr4IL0P/Qeqqo8CbwW2Jvk+8FXgrObww+i9Yd9B7zTY7fRWcjN5H3AF8H/pXRl3fvMa+4C19C5omKK3SnkDLf17rqrd9D73upTeCu5F9C79P1BVB+hdEHIuvZ/rFcB/63vuBL3Pod7ZHN/T9JUOK35hodQ9Sa4F3l9VfzrXtUhzxRWUJKmTDChJUid5ik+S1EmuoCRJndS5m1sezqJFi2r58uVzXYYk6Qh98YtfvK2qxg7Xb+QCavny5UxMTMx1GZKkI5RkpvtD/hRP8UmSOsmAkiR1kgElSeokA0qS1EkGlCSpkwwoSVInGVCSpE4yoCRJnWRASZI6yYCSJHXSyN3qSJK6bPmmT8x1Ca269Q9fcMxey4CSDsM3HGlueIpPktRJBpQkqZMMKElSJ/kZlH7Cz1p0f/j7orbNy4DyH5YkdZ+n+CRJnWRASZI6yYCSJHVSqwGVZHWS3Un2JNk04PgpST6b5MtJbkry/DbrkSSNjtYCKskCYDNwFrAKWJ9k1bRubwKuqqpTgXXAf26rHknSaGlzBXU6sKeq9lbVAWArsHZanwIe1myfDOxvsR5J0ghpM6AWA/v62pPNvn4XA+ckmQS2A68bNFCSDUkmkkxMTU21UaskqWPaDKgM2FfT2uuBK6pqCfB84H1J7lNTVW2pqvGqGh8bG2uhVElS17QZUJPA0r72Eu57Cu884CqAqroOOBFY1GJNkqQR0WZA7QRWJlmRZCG9iyC2TevzDeC5AEkeTy+gPIcnSWovoKrqILAR2AHcQu9qvV1JLkmypul2IfDqJDcCVwLnVtX004CSpHmo1XvxVdV2ehc/9O+7qG/7ZuBX26xBkjSavJOEJKmTDChJUicZUJKkTjKgJEmdZEBJkjrJgJIkdZIBJUnqJANKktRJBpQkqZMMKElSJxlQkqROMqAkSZ1kQEmSOsmAkiR1kgElSeokA0qS1EkGlCSpk1oNqCSrk+xOsifJpgHH357khubxv5N8t816JEmjo7WvfE+yANgMPA+YBHYm2dZ8zTsAVfWv+/q/Dji1rXokSaOlzRXU6cCeqtpbVQeArcDaWfqvB65ssR5J0ghpM6AWA/v62pPNvvtIsgxYAXxmhuMbkkwkmZiamjrqhUqSuqfNgMqAfTVD33XA1VV1z6CDVbWlqsaranxsbOyoFShJ6q42A2oSWNrXXgLsn6HvOjy9J0nq02ZA7QRWJlmRZCG9ENo2vVOSXwJ+HriuxVokSSOmtYCqqoPARmAHcAtwVVXtSnJJkjV9XdcDW6tqptN/kqR5qLXLzAGqajuwfdq+i6a1L26zBknSaPJOEpKkTjKgJEmdZEBJkjrJgJIkdZIBJUnqJANKktRJBpQkqZMMKElSJxlQkqROMqAkSZ1kQEmSOsmAkiR1kgElSeokA0qS1EkGlCSpkwwoSVInGVCSpE5qNaCSrE6yO8meJJtm6PPyJDcn2ZXkg23WI0kaHa195XuSBcBm4HnAJLAzybaqurmvz0rgjcCvVtUdSR7ZVj2SpNHS5grqdGBPVe2tqgPAVmDttD6vBjZX1R0AVfXtFuuRJI2QNgNqMbCvrz3Z7Ov3OOBxST6X5PokqwcNlGRDkokkE1NTUy2VK0nqkjYDKgP21bT2CcBK4NnAeuBPkzz8Pk+q2lJV41U1PjY2dtQLlSR1T5sBNQks7WsvAfYP6PPnVXV3Vf0NsJteYEmS5rk2A2onsDLJiiQLgXXAtml9rgGeA5BkEb1TfntbrEmSNCJaC6iqOghsBHYAtwBXVdWuJJckWdN02wHcnuRm4LPAG6rq9rZqkiSNjtYuMweoqu3A9mn7LurbLuCC5iFJ0k94JwlJUicZUJKkTjKgJEmdZEBJkjrJgJIkdZIBJUnqJANKktRJBpQkqZMMKElSJxlQkqROMqAkSZ1kQEmSOsmAkiR1kgElSeokA0qS1EmHDagkD07ye0ne1bRXJnlh+6VJkuazYVZQfwbcBfzDpj0JvLm1iiRJYriAemxVvQ24G6CqfgRkmMGTrE6yO8meJJsGHD83yVSSG5rHq+5X9ZKk49YwX/l+IMmDgAJI8lh6K6pZJVkAbAaeR2/VtTPJtqq6eVrXD1XVxvtXtiTpeDfMCurfAf8dWJrkA8CngX8zxPNOB/ZU1d6qOgBsBdYecaWSpHll1oBKEuB/AS8BzgWuBMar6tohxl4M7OtrTzb7pntpkpuSXJ1k6Qx1bEgykWRiampqiJeWJI26WQOqqgq4pqpur6pPVNXHq+q2Icce9DlVTWt/DFheVU8CPgW8Z4Y6tlTVeFWNj42NDfnykqRRNswpvuuTnHYEY08C/SuiJcD+/g5N8B36POtdwFOP4HUkScehYQLqOcB1Sb7WnIr7SpKbhnjeTmBlkhVJFgLrgG39HZI8qq+5Brhl2MIlSce3Ya7iO+tIBq6qg0k2AjuABcDlVbUrySXARFVtA85PsgY4CHyH3udckiQdPqCq6utJngz8o2bXX1XVjcMMXlXbge3T9l3Ut/1G4I3DlytJmi+GudXR64EPAI9sHu9P8rq2C5MkzW/DnOI7Dzijqu4ESPJW4Drg0jYLkyTNb8NcJBHgnr72PQx5qyNJko7UMCuoPwO+kOSjTfvFwLvbK0mSpOEukvjjJNcCz6C3cnplVX257cIkSfPbYQMqydOAXVX1paZ9UpIzquoLrVcnSZq3hvkM6r8AP+hr39nskySpNUNdJNHckw+AqrqX4T67kiTpiA0TUHuTnJ/kgc3j9cDetguTJM1vwwTUa4CnA99sHmcAG9osSpKkYa7i+za9G71KknTMzLiCSvLqJCub7SS5PMn3mjua/8qxK1GSNB/Ndorv9cCtzfZ64MnAY4ALgD9ptyxJ0nw3W0AdrKq7m+0XAu9tvmDwU8BD2i9NkjSfzRZQ9yZ5VJITgefS+0r2Qx7UblmSpPlutoskLgIm6H3Z4Laq2gWQ5Fl4mbkkqWUzBlRVfTzJMuCkqrqj79AE8IrWK5MkzWuz/h1UVR2cFk5U1Z1V9YOZntMvyeoku5PsSbJpln5nJ6kk48OVLUk63g3zh7pHJMkCYDNwFrAKWJ9k1YB+JwHnA958VpL0E60FFHA6sKeq9lbVAWArsHZAv98H3gb8uMVaJEkj5ogCKskvD9FtMbCvrz3Z7Osf51RgaVV9/DCvtyHJRJKJqamp+12vJGn0HOkK6i+G6DPoa+F/clf0JA8A3g5ceLiBqmpLVY1X1fjY2NjwVUqSRtaMV/ElecdMh4CHDzH2JLC0r70E2N/XPgl4InBtEoC/B2xLsqaqJoYYX5J0HJvt76BeSW91c9eAY+uHGHsnsDLJCnp3QV8H/LNDB6vqe8CiQ+3ma+V/23CSJMHsAbUT+GpVfX76gSQXH27gqjqYZCOwg94f+15eVbuSXAJMVNW2I6xZkjQPzBZQZzPDlXVVtWKYwatqO7B92r6LZuj77GHGlCTND7NdJPHQqvrhMatEkqQ+swXUNYc2knzkGNQiSdJPzBZQ/ZeJP6btQiRJ6jdbQNUM25IktW62iySenOT79FZSD2q2adpVVQ9rvTpJ0rw129dtLDiWhUiS1K/Nm8VKknTEDChJUicZUJKkTjKgJEmdZEBJkjrJgJIkdZIBJUnqJANKktRJBpQkqZMMKElSJxlQkqROajWgkqxOsjvJniSbBhx/TZKvJLkhyV8nWdVmPZKk0dFaQCVZAGwGzgJWAesHBNAHq+ofVNVTgLcBf9xWPZKk0dLmCup0YE9V7a2qA8BWYG1/h6r6fl/zIfi9U5KkxmzfB/WzWgzs62tPAmdM75TkXwIXAAuBfzxooCQbgA0Ap5xyylEvVJLUPW2uoDJg331WSFW1uaoeC/xb4E2DBqqqLVU1XlXjY2NjR7lMSVIXtRlQk8DSvvYSYP8s/bcCL26xHknSCGkzoHYCK5OsSLIQWAds6++QZGVf8wXA/2mxHknSCGntM6iqOphkI7ADWABcXlW7klwCTFTVNmBjkjOBu4E7gN9oqx5J0mhp8yIJqmo7sH3avov6tl/f5utLkkaXd5KQJHWSASVJ6iQDSpLUSQaUJKmTDChJUicZUJKkTjKgJEmdZEBJkjrJgJIkdZIBJUnqJANKktRJBpQkqZMMKElSJxlQkqROMqAkSZ1kQEmSOsmAkiR1UqsBlWR1kt1J9iTZNOD4BUluTnJTkk8nWdZmPZKk0dFaQCVZAGwGzgJWAeuTrJrW7cvAeFU9CbgaeFtb9UiSRkubK6jTgT1VtbeqDgBbgbX9Harqs1X1w6Z5PbCkxXokSSOkzYBaDOzra082+2ZyHvDJFuuRJI2QE1ocOwP21cCOyTnAOPCsGY5vADYAnHLKKUerPklSh7W5gpoElva1lwD7p3dKcibwu8Caqrpr0EBVtaWqxqtqfGxsrJViJUnd0mZA7QRWJlmRZCGwDtjW3yHJqcBl9MLp2y3WIkkaMa0FVFUdBDYCO4BbgKuqaleSS5Ksabr9EfBQ4MNJbkiybYbhJEnzTJufQVFV24Ht0/Zd1Ld9ZpuvL0kaXd5JQpLUSQaUJKmTDChJUicZUJKkTjKgJEmdZEBJkjrJgJIkdZIBJUnqJANKktRJBpQkqZMMKElSJxlQkqROMqAkSZ1kQEmSOsmAkiR1kgElSeokA0qS1EmtBlSS1Ul2J9mTZNOA489M8qUkB5Oc3WYtkqTR0lpAJVkAbAbOAlYB65OsmtbtG8C5wAfbqkOSNJpOaHHs04E9VbUXIMlWYC1w86EOVXVrc+zeFuuQJI2gNk/xLQb29bUnm32SJB1WmwGVAfvqiAZKNiSZSDIxNTX1M5YlSRoFbQbUJLC0r70E2H8kA1XVlqoar6rxsbGxo1KcJKnb2gyoncDKJCuSLATWAdtafD1J0nGktYCqqoPARmAHcAtwVVXtSnJJkjUASU5LMgm8DLgsya626pEkjZY2r+KjqrYD26ftu6hveye9U3+SJP0U7yQhSeokA0qS1EkGlCSpkwwoSVInGVCSpE4yoCRJnWRASZI6yYCSJHWSASVJ6iQDSpLUSQaUJKmTDChJUicZUJKkTjKgJEmdZEBJkjrJgJIkdZIBJUnqJANKktRJrQZUktVJdifZk2TTgOM/l+RDzfEvJFneZj2SpNHRWkAlWQBsBs4CVgHrk6ya1u084I6q+vvA24G3tlWPJGm0tLmCOh3YU1V7q+oAsBVYO63PWuA9zfbVwHOTpMWaJEkjIlXVzsDJ2cDqqnpV0/414Iyq2tjX56tNn8mm/bWmz23TxtoAbGiavwTsbqXo9iwCbjtsr/nHeRnMeRnMeRlsFOdlWVWNHa7TCS0WMGglND0Nh+lDVW0BthyNouZCkomqGp/rOrrGeRnMeRnMeRnseJ6XNk/xTQJL+9pLgP0z9UlyAnAy8J0Wa5IkjYg2A2onsDLJiiQLgXXAtml9tgG/0WyfDXym2jrnKEkaKa2d4quqg0k2AjuABcDlVbUrySXARFVtA94NvC/JHnorp3Vt1TPHRvb0ZMucl8Gcl8Gcl8GO23lp7SIJSZJ+Ft5JQpLUSQaUJKmTDKijaIhbOz0zyZeSHGz+TmxeGGJeLkhyc5Kbknw6ybK5qPNYO9y89PU7O0klOS4vJZ5uiN+Xc5NMJbmhebxqLuo81oaYl1OSfDbJl5t/S8+fizqPqqrycRQe9C4E+RrwGGAhcCOwalqf5cCTgPcCZ891zR2al+cAD262Xwt8aK7r7sK8NP1OAv4HcD0wPtd1d2FegHOBd851rR2cly3Aa5vtVcCtc133z/pwBXX0HPbWTlV1a1XdBNw7FwXOkWHm5bNV9cOmeT29v5k73g1zKzCA3wfeBvz4WBY3h4adl/lmmHkp4GHN9snc9+9OR44BdfQsBvb1tSebffPd/Z2X84BPtlpRNxx2XpKcCiytqo8fy8Lm2LC/Ly9tTmNdnWTpgOPHm2Hm5WLgnCSTwHbgdcemtPYYUEfPULdtmoeGnpck5wDjwB+1WlE3zDovSR5A7w7/Fx6zirphmN+XjwHLq+pJwKf4/zecPp4NMy/rgSuqagnwfHp/YzrS7/EjXXzHDHNrp/loqHlJcibwu8CaqrrrGNU2lw43LycBTwSuTXIr8DRg2zy4UOKwvy9VdXvf78i7gKceo9rm0jD/js4DrgKoquuAE+ndSHZkGVBHzzC3dpqPDjsvzamsy+iF07fnoMa5MOu8VNX3qmpRVS2vquX0PptbU1UTc1PuMTPM78uj+pprgFuOYX1zZZj3l28AzwVI8nh6ATV1TKs8ygyoo6SqDgKHbu10C3BVNbd2SrIGIMlpzfnhlwGXJdk1dxUfG8PMC71Teg8FPtxcNnzcB/uQ8zLvDDkv5yfZleRG4Hx6V/Ud14aclwuBVzfzciVwbjWX9I0qb3UkSeokV1CSpE4yoCRJnWRASZI6yYCSJHWSASVJ6iQDSpLUSQaUJKmT/h9lj7Rtud1E7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b196ce8be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(5)\n",
    "\n",
    "rects1 = ax.bar(index, f1_scores)\n",
    "\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Scores per model')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([0.1, 0.2, 0.4, 0.5, 0.8])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Like the amount of nodes in the hidden layer, the amount of dropout seems to be arbitrary as well. It might be because the dataset is relatively small, so it never gets a chance to overfit.\n",
    "\n",
    "We'll choose a dropout that's somewhere in the middle of the range; **0.4**, which achieved an F1 score of *0.844*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn_image_classification]",
   "language": "python",
   "name": "conda-env-cnn_image_classification-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
