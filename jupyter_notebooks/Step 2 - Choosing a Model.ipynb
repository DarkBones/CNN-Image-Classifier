{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Choosing a Model\n",
    "\n",
    "In this notebook, I have documented how different models are trained and how I determined which of the models achieved the best results.\n",
    "\n",
    "I trained the following models:\n",
    "- **neuralnet_model:** A traditional feed-forward neural network for benchmarking against the performance of Convolutional Neural Networks\n",
    "- **cnn_model:** A convolutional neural network trained from scratch\n",
    "- **VGG19:** A *pre-trained* convolutional neural network for transfer-learning\n",
    "- **ResNet50:** A *pre-trained* convolutional neural network for transfer-learning\n",
    "- **InceptionV3:** A *pre-trained* convolutional neural network for transfer-learning\n",
    "\n",
    "All 5 models are trained and saved in a list. After training the models, we iterate through the list and determine the models' respective F1 scores with the following formula:\n",
    "![title](notebook_images/f1_formula.jpg)\n",
    "\n",
    "We will use the training and validation data to train the models described above. Afterwards, we use the testing set (which the models haven't seen before) to calculate their F1 scores. The model with the highest F1 score on the testing set, is the model we're going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "from image_preprocessor import ImagePreprocessor\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image as Image\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "\n",
    "The parameters below are used throughout the rest of the code. They describe where the images are located and how the datasets should be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = os.path.join('..', 'application', 'images')\n",
    "originals_dir = os.path.join(root_dir, \"original\")\n",
    "training_dir = os.path.join(root_dir, \"train\")\n",
    "test_dir = os.path.join(root_dir, \"test\")\n",
    "val_dir = os.path.join(root_dir, \"validation\")\n",
    "\n",
    "target_imagesize = (256, 256)\n",
    "\n",
    "clear_existing_data = True # if true, data in training, test and validation directories will be deleted before splitting the data in the originals directory\n",
    "augment_data = True # whether images should be augmented during preprocessing\n",
    "augmentations = 20 # how many augmentations to make for each original image\n",
    "\n",
    "random_seed = 7\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "saved_models_dir = os.path.join('..', 'application', 'saved_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate preprocessor and preprocess the dataset\n",
    "\n",
    "In the code below, we instantiate and run the *Image Preprocessor* class. This class takes the images from the original dataset and splits them into training, validation, and test sets. This is described in more detail in the [Image Preprocessing](Step 1 - Image Preprocessing.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 image categories\n",
      "465 total images\n",
      "\n",
      "279 training images\n",
      "93 validation images\n",
      "93 test images\n",
      "\n",
      "Categories:\n",
      "  - animal\n",
      "  - city_scape\n",
      "  - food\n",
      "  - group\n",
      "  - landscape\n",
      "  - me\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ImagePreprocessor()\n",
    "preprocessor.root_dir = root_dir\n",
    "preprocessor.originals_dir = originals_dir\n",
    "preprocessor.training_dir = training_dir\n",
    "preprocessor.test_dir = test_dir\n",
    "preprocessor.val_dir = val_dir\n",
    "preprocessor.random_seed = random_seed\n",
    "preprocessor.target_imagesize = target_imagesize\n",
    "preprocessor.clear_existing_data = clear_existing_data\n",
    "\n",
    "preprocessor.initialize()\n",
    "categories = preprocessor.categories\n",
    "training_count = preprocessor.training_count\n",
    "validation_count = preprocessor.validation_count\n",
    "test_count = preprocessor.test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate augmented images\n",
    "\n",
    "Because our dataset is relatively small (only a few hundred images), we will artificially generate more data by augmenting the images. The original images are randomly rotated, zoomed and / or flipped horizontally.\n",
    "\n",
    "![title](notebook_images/image_augmentation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 279 images belonging to 6 classes.\n",
      "Found 93 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "img_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255,\n",
    "    fill_mode='reflect')\n",
    "\n",
    "train_generator = img_datagen.flow_from_directory(training_dir,\n",
    "                                                   target_size=target_imagesize,\n",
    "                                                   batch_size=augmentations,\n",
    "                                                   shuffle=True,\n",
    "                                                   seed=random_seed)\n",
    "\n",
    "validation_generator = img_datagen.flow_from_directory(val_dir,\n",
    "                                                   target_size=target_imagesize,\n",
    "                                                   batch_size=augmentations,\n",
    "                                                   shuffle=True,\n",
    "                                                   seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark model: Random guessing\n",
    "\n",
    "To get a better understanding of how well our models work, we'll first create a model that just randomly guesses the categories and calculate its F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_guesses(count):\n",
    "    guesses = []\n",
    "    for i in range(count):\n",
    "        guesses.append(random.randint(0, len(categories)))\n",
    "    return guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the F1 score\n",
    "\n",
    "To measure how good the model is at predicting the correct category for images, we calculate the model's F1 score. The higher this score, the better the model is performing. In the end, we choose the model that has the highest F1 score on the images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_category(img_path, pred_model):\n",
    "    img_tensor = preprocessor.file_to_tensor(img_path)\n",
    "    try:\n",
    "        h = pred_model.predict(img_tensor)\n",
    "    except:\n",
    "        img_tensor = img_tensor.reshape(1,x_length)\n",
    "        h = pred_model.predict(img_tensor)\n",
    "    return categories[np.argmax(h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_score_cal(model=None):\n",
    "    test_images = np.array(glob(os.path.join(test_dir, \"*\", \"*\")))\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for img in test_images:\n",
    "        y_true.append(categories.index(re.split(r'[\\\\/]',img)[-2]))\n",
    "        if model != None:\n",
    "            pred = predict_category(img, model)\n",
    "            y_pred.append(categories.index(pred))\n",
    "        \n",
    "    if model == None:\n",
    "        y_pred = random_guesses(len(y_true))\n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We'll store the trained models in a list called *models*, along with another list of the model names so we can refer to them later. After all models are trained, we'll calculate their *f1 scores* and store them in the f1_scores list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "modelnames = []\n",
    "f1_scores = []\n",
    "\n",
    "models.append(None)\n",
    "modelnames.append(\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark model: Traditional Neural Network\n",
    "\n",
    "To test the effectiveness of Convolutional Neural Networks, we'll also create a traditional Feed-Forward Neural Network to see how it compares against CNN's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donke\\Anaconda2\\envs\\cnn_image_classification\\lib\\site-packages\\PIL\\Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    }
   ],
   "source": [
    "# load the file locations and their labels 'y'\n",
    "train_files, y_train = preprocessor.load_dataset(training_dir)\n",
    "val_files, y_val = preprocessor.load_dataset(val_dir)\n",
    "test_files, y_test = preprocessor.load_dataset(test_dir)\n",
    "\n",
    "# load training, validation, and test matrices\n",
    "x_train = preprocessor.files_to_tensors(train_files)\n",
    "x_val = preprocessor.files_to_tensors(val_files)\n",
    "x_test = preprocessor.files_to_tensors(test_files)\n",
    "\n",
    "# calculate how many pixels are in the images\n",
    "x_length = 1\n",
    "for n in x_train.shape[1:]:\n",
    "    x_length *= n\n",
    "\n",
    "# reshape the tensors into single dimensions\n",
    "x_train = x_train.reshape(len(x_train),x_length)\n",
    "x_val = x_val.reshape(len(x_val),x_length)\n",
    "x_test = x_test.reshape(len(x_test),x_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               100663808 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 101,192,198\n",
      "Trainable params: 101,192,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neuralnet_model = Sequential()\n",
    "neuralnet_model.add(Dense(512, input_dim=x_length, activation='relu'))\n",
    "neuralnet_model.add(Dense(512, activation='relu'))\n",
    "neuralnet_model.add(Dense(512, activation='relu'))\n",
    "neuralnet_model.add(Dropout(0.3))\n",
    "neuralnet_model.add(Dense(len(categories), activation='softmax'))\n",
    "neuralnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 279 samples, validate on 93 samples\n",
      "Epoch 1/10\n",
      "250/279 [=========================>....] - ETA: 0s - loss: 1.8501 - acc: 0.1720Epoch 00000: val_loss improved from inf to 1.71299, saving model to saved_models\\traditional_neuralnet.hdf5\n",
      "279/279 [==============================] - 7s - loss: 1.8474 - acc: 0.1828 - val_loss: 1.7130 - val_acc: 0.3441\n",
      "Epoch 2/10\n",
      "250/279 [=========================>....] - ETA: 0s - loss: 1.6875 - acc: 0.3360Epoch 00001: val_loss improved from 1.71299 to 1.65505, saving model to saved_models\\traditional_neuralnet.hdf5\n",
      "279/279 [==============================] - 11s - loss: 1.6864 - acc: 0.3262 - val_loss: 1.6550 - val_acc: 0.3011\n",
      "Epoch 3/10\n",
      "250/279 [=========================>....] - ETA: 0s - loss: 1.5784 - acc: 0.3520Epoch 00002: val_loss improved from 1.65505 to 1.59954, saving model to saved_models\\traditional_neuralnet.hdf5\n",
      "279/279 [==============================] - 6s - loss: 1.5646 - acc: 0.3656 - val_loss: 1.5995 - val_acc: 0.3763\n",
      "Epoch 4/10\n",
      "250/279 [=========================>....] - ETA: 0s - loss: 1.4769 - acc: 0.4720Epoch 00003: val_loss improved from 1.59954 to 1.54282, saving model to saved_models\\traditional_neuralnet.hdf5\n",
      "279/279 [==============================] - 7s - loss: 1.4757 - acc: 0.4695 - val_loss: 1.5428 - val_acc: 0.4086\n",
      "Epoch 5/10\n",
      "250/279 [=========================>....] - ETA: 0s - loss: 1.3790 - acc: 0.5200Epoch 00004: val_loss improved from 1.54282 to 1.50386, saving model to saved_models\\traditional_neuralnet.hdf5\n",
      "279/279 [==============================] - 6s - loss: 1.4024 - acc: 0.4982 - val_loss: 1.5039 - val_acc: 0.4086\n",
      "Epoch 6/10\n",
      "250/279 [=========================>....] - ETA: 0s - loss: 1.2837 - acc: 0.5600Epoch 00005: val_loss improved from 1.50386 to 1.46535, saving model to saved_models\\traditional_neuralnet.hdf5\n",
      "279/279 [==============================] - 7s - loss: 1.2988 - acc: 0.5484 - val_loss: 1.4653 - val_acc: 0.4086\n",
      "Epoch 7/10\n",
      "250/279 [=========================>....] - ETA: 0s - loss: 1.2050 - acc: 0.5680Epoch 00006: val_loss improved from 1.46535 to 1.45324, saving model to saved_models\\traditional_neuralnet.hdf5\n",
      "279/279 [==============================] - 6s - loss: 1.2091 - acc: 0.5627 - val_loss: 1.4532 - val_acc: 0.3548\n",
      "Epoch 8/10\n",
      "250/279 [=========================>....] - ETA: 0s - loss: 1.1922 - acc: 0.5840Epoch 00007: val_loss did not improve\n",
      "279/279 [==============================] - 0s - loss: 1.1896 - acc: 0.5842 - val_loss: 1.4559 - val_acc: 0.3871\n",
      "Epoch 9/10\n",
      "250/279 [=========================>....] - ETA: 0s - loss: 1.1568 - acc: 0.5840Epoch 00008: val_loss did not improve\n",
      "279/279 [==============================] - 0s - loss: 1.1474 - acc: 0.5806 - val_loss: 1.4601 - val_acc: 0.4194\n",
      "Epoch 10/10\n",
      "250/279 [=========================>....] - ETA: 0s - loss: 1.1152 - acc: 0.6200Epoch 00009: val_loss improved from 1.45324 to 1.43180, saving model to saved_models\\traditional_neuralnet.hdf5\n",
      "279/279 [==============================] - 4s - loss: 1.0852 - acc: 0.6380 - val_loss: 1.4318 - val_acc: 0.3871\n"
     ]
    }
   ],
   "source": [
    "neuralnet_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=os.path.join(saved_models_dir, 'traditional_neuralnet.hdf5'), \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "neuralnet_model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer])\n",
    "\n",
    "neuralnet_model.load_weights(filepath=os.path.join(saved_models_dir,'traditional_neuralnet.hdf5'))\n",
    "models.append(neuralnet_model)\n",
    "modelnames.append(\"benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Construct a new CNN model from scratch\n",
    "\n",
    "In the code below, we'll create a new Convolutional Neural Network from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               16777728  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 17,086,902\n",
      "Trainable params: 17,086,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(filters=16,\n",
    "                     kernel_size=2, \n",
    "                     strides=(1, 1), \n",
    "                     padding='same', \n",
    "                     activation='relu', \n",
    "                     input_shape=(target_imagesize[0], target_imagesize[1], 3)))\n",
    "cnn_model.add(MaxPooling2D(pool_size=2))\n",
    "cnn_model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=2))\n",
    "cnn_model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=2))\n",
    "cnn_model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=2))\n",
    "cnn_model.add(Dropout(0.3))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(512, activation='relu'))\n",
    "cnn_model.add(Dropout(0.3))\n",
    "cnn_model.add(Dense(512, activation='relu'))\n",
    "cnn_model.add(Dropout(0.3))\n",
    "cnn_model.add(Dense(len(categories), activation='softmax'))\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.7636 - acc: 0.2271"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donke\\Anaconda2\\envs\\cnn_image_classification\\lib\\site-packages\\PIL\\Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 1.73376, saving model to saved_models\\cnn_from_scratch.hdf5\n",
      "279/279 [==============================] - 362s - loss: 1.7635 - acc: 0.2275 - val_loss: 1.7338 - val_acc: 0.2670\n",
      "Epoch 2/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.7194 - acc: 0.2856Epoch 00001: val_loss improved from 1.73376 to 1.70591, saving model to saved_models\\cnn_from_scratch.hdf5\n",
      "279/279 [==============================] - 351s - loss: 1.7190 - acc: 0.2861 - val_loss: 1.7059 - val_acc: 0.3532\n",
      "Epoch 3/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.6931 - acc: 0.3206Epoch 00002: val_loss improved from 1.70591 to 1.67633, saving model to saved_models\\cnn_from_scratch.hdf5\n",
      "279/279 [==============================] - 350s - loss: 1.6932 - acc: 0.3201 - val_loss: 1.6763 - val_acc: 0.3677\n",
      "Epoch 4/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.6509 - acc: 0.3510Epoch 00003: val_loss improved from 1.67633 to 1.62468, saving model to saved_models\\cnn_from_scratch.hdf5\n",
      "279/279 [==============================] - 348s - loss: 1.6510 - acc: 0.3512 - val_loss: 1.6247 - val_acc: 0.3770\n",
      "Epoch 5/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.5867 - acc: 0.3877Epoch 00004: val_loss improved from 1.62468 to 1.53539, saving model to saved_models\\cnn_from_scratch.hdf5\n",
      "279/279 [==============================] - 355s - loss: 1.5868 - acc: 0.3876 - val_loss: 1.5354 - val_acc: 0.4204\n",
      "Epoch 6/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.4922 - acc: 0.4210Epoch 00005: val_loss improved from 1.53539 to 1.44360, saving model to saved_models\\cnn_from_scratch.hdf5\n",
      "279/279 [==============================] - 365s - loss: 1.4917 - acc: 0.4207 - val_loss: 1.4436 - val_acc: 0.4083\n",
      "Epoch 7/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.4157 - acc: 0.4334Epoch 00006: val_loss improved from 1.44360 to 1.39263, saving model to saved_models\\cnn_from_scratch.hdf5\n",
      "279/279 [==============================] - 364s - loss: 1.4156 - acc: 0.4336 - val_loss: 1.3926 - val_acc: 0.4395\n",
      "Epoch 8/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.3658 - acc: 0.4532Epoch 00007: val_loss improved from 1.39263 to 1.36792, saving model to saved_models\\cnn_from_scratch.hdf5\n",
      "279/279 [==============================] - 365s - loss: 1.3653 - acc: 0.4536 - val_loss: 1.3679 - val_acc: 0.4702\n",
      "Epoch 9/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.3328 - acc: 0.4743Epoch 00008: val_loss improved from 1.36792 to 1.35218, saving model to saved_models\\cnn_from_scratch.hdf5\n",
      "279/279 [==============================] - 376s - loss: 1.3321 - acc: 0.4749 - val_loss: 1.3522 - val_acc: 0.4569\n",
      "Epoch 10/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 1.2985 - acc: 0.4876Epoch 00009: val_loss improved from 1.35218 to 1.31247, saving model to saved_models\\cnn_from_scratch.hdf5\n",
      "279/279 [==============================] - 366s - loss: 1.2980 - acc: 0.4878 - val_loss: 1.3125 - val_acc: 0.4862\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=os.path.join(saved_models_dir,'cnn_from_scratch.hdf5'), \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "cnn_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])\n",
    "cnn_model.fit_generator(train_generator,\n",
    "                        steps_per_epoch=training_count, \n",
    "                        epochs=epochs,\n",
    "                        validation_data = validation_generator,\n",
    "                        validation_steps=validation_count,\n",
    "                        callbacks=[checkpointer],\n",
    "                        verbose=1)\n",
    "\n",
    "cnn_model.load_weights(filepath=os.path.join(saved_models_dir,'cnn_from_scratch.hdf5'))\n",
    "models.append(cnn_model)\n",
    "modelnames.append(\"new cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "Transfer learning is taking an existing, pre trained model and stripping off the final layer and replacing it with our own layers so it can classify the objects we're looking for. We'll use three different pre-trained networks to see how they compare:\n",
    "- VGG19\n",
    "- ResNet50\n",
    "- InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_models = []\n",
    "base_models.append(VGG19(include_top=False, weights = 'imagenet', input_shape = (target_imagesize[0], target_imagesize[1], 3)))\n",
    "base_models.append(ResNet50(include_top=False, weights = 'imagenet', input_shape = (target_imagesize[0], target_imagesize[1], 3)))\n",
    "base_models.append(InceptionV3(include_top=False, weights = 'imagenet', input_shape = (target_imagesize[0], target_imagesize[1], 3)))\n",
    "\n",
    "base_modelnames = \"VGG19 ResNet50 InceptionV3\".split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: VGG19\n",
      "Epoch 1/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.8404 - acc: 0.2176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donke\\Anaconda2\\envs\\cnn_image_classification\\lib\\site-packages\\PIL\\Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 1.79143, saving model to saved_models\\VGG19.hdf5\n",
      "27/27 [==============================] - 39s - loss: 1.8390 - acc: 0.2170 - val_loss: 1.7914 - val_acc: 0.2312\n",
      "Epoch 2/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7935 - acc: 0.2373Epoch 00001: val_loss improved from 1.79143 to 1.79095, saving model to saved_models\\VGG19.hdf5\n",
      "27/27 [==============================] - 28s - loss: 1.7931 - acc: 0.2378 - val_loss: 1.7909 - val_acc: 0.2486\n",
      "Epoch 3/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7910 - acc: 0.2545Epoch 00002: val_loss improved from 1.79095 to 1.79031, saving model to saved_models\\VGG19.hdf5\n",
      "27/27 [==============================] - 29s - loss: 1.7910 - acc: 0.2524 - val_loss: 1.7903 - val_acc: 0.2659\n",
      "Epoch 4/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7938 - acc: 0.2257Epoch 00003: val_loss improved from 1.79031 to 1.78992, saving model to saved_models\\VGG19.hdf5\n",
      "27/27 [==============================] - 29s - loss: 1.7935 - acc: 0.2266 - val_loss: 1.7899 - val_acc: 0.2601\n",
      "Epoch 5/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7914 - acc: 0.2496Epoch 00004: val_loss improved from 1.78992 to 1.78975, saving model to saved_models\\VGG19.hdf5\n",
      "27/27 [==============================] - 28s - loss: 1.7913 - acc: 0.2551 - val_loss: 1.7898 - val_acc: 0.2370\n",
      "Epoch 6/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7908 - acc: 0.2606Epoch 00005: val_loss improved from 1.78975 to 1.78902, saving model to saved_models\\VGG19.hdf5\n",
      "27/27 [==============================] - 28s - loss: 1.7910 - acc: 0.2602 - val_loss: 1.7890 - val_acc: 0.2543\n",
      "Epoch 7/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7902 - acc: 0.2356Epoch 00006: val_loss improved from 1.78902 to 1.78855, saving model to saved_models\\VGG19.hdf5\n",
      "27/27 [==============================] - 29s - loss: 1.7902 - acc: 0.2362 - val_loss: 1.7886 - val_acc: 0.2486\n",
      "Epoch 8/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7889 - acc: 0.2507Epoch 00007: val_loss improved from 1.78855 to 1.78838, saving model to saved_models\\VGG19.hdf5\n",
      "27/27 [==============================] - 28s - loss: 1.7889 - acc: 0.2544 - val_loss: 1.7884 - val_acc: 0.2428\n",
      "Epoch 9/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7878 - acc: 0.2454Epoch 00008: val_loss improved from 1.78838 to 1.78769, saving model to saved_models\\VGG19.hdf5\n",
      "27/27 [==============================] - 29s - loss: 1.7875 - acc: 0.2493 - val_loss: 1.7877 - val_acc: 0.2543\n",
      "Epoch 10/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7873 - acc: 0.2358Epoch 00009: val_loss improved from 1.78769 to 1.78730, saving model to saved_models\\VGG19.hdf5\n",
      "27/27 [==============================] - 32s - loss: 1.7875 - acc: 0.2363 - val_loss: 1.7873 - val_acc: 0.2486\n",
      "\n",
      "Training Model: ResNet50\n",
      "Epoch 1/10\n",
      "26/27 [===========================>..] - ETA: 1s - loss: 1.9393 - acc: 0.1502Epoch 00000: val_loss improved from inf to 1.79381, saving model to saved_models\\ResNet50.hdf5\n",
      "27/27 [==============================] - 46s - loss: 1.9345 - acc: 0.1558 - val_loss: 1.7938 - val_acc: 0.2139\n",
      "Epoch 2/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.8182 - acc: 0.2008Epoch 00001: val_loss improved from 1.79381 to 1.79379, saving model to saved_models\\ResNet50.hdf5\n",
      "27/27 [==============================] - 27s - loss: 1.8145 - acc: 0.2063 - val_loss: 1.7938 - val_acc: 0.1676\n",
      "Epoch 3/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.8064 - acc: 0.2275Epoch 00002: val_loss improved from 1.79379 to 1.78091, saving model to saved_models\\ResNet50.hdf5\n",
      "27/27 [==============================] - 29s - loss: 1.7992 - acc: 0.2321 - val_loss: 1.7809 - val_acc: 0.1792\n",
      "Epoch 4/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7406 - acc: 0.2759Epoch 00003: val_loss did not improve\n",
      "27/27 [==============================] - 28s - loss: 1.7393 - acc: 0.2750 - val_loss: 1.7913 - val_acc: 0.1618\n",
      "Epoch 5/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.6872 - acc: 0.3263Epoch 00004: val_loss did not improve\n",
      "27/27 [==============================] - 28s - loss: 1.6866 - acc: 0.3253 - val_loss: 1.7857 - val_acc: 0.1618\n",
      "Epoch 6/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.6623 - acc: 0.3605Epoch 00005: val_loss improved from 1.78091 to 1.77466, saving model to saved_models\\ResNet50.hdf5\n",
      "27/27 [==============================] - 27s - loss: 1.6616 - acc: 0.3602 - val_loss: 1.7747 - val_acc: 0.1850\n",
      "Epoch 7/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.6185 - acc: 0.3666Epoch 00006: val_loss did not improve\n",
      "27/27 [==============================] - 28s - loss: 1.6156 - acc: 0.3660 - val_loss: 1.7858 - val_acc: 0.1676\n",
      "Epoch 8/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.5851 - acc: 0.3919Epoch 00007: val_loss did not improve\n",
      "27/27 [==============================] - 29s - loss: 1.5874 - acc: 0.3885 - val_loss: 1.8041 - val_acc: 0.1676\n",
      "Epoch 9/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.5299 - acc: 0.4245Epoch 00008: val_loss did not improve\n",
      "27/27 [==============================] - 28s - loss: 1.5321 - acc: 0.4254 - val_loss: 1.8132 - val_acc: 0.1734\n",
      "Epoch 10/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.4540 - acc: 0.4904Epoch 00009: val_loss did not improve\n",
      "27/27 [==============================] - 28s - loss: 1.4505 - acc: 0.4889 - val_loss: 1.8266 - val_acc: 0.1618\n",
      "\n",
      "Training Model: InceptionV3\n",
      "Epoch 1/10\n",
      "26/27 [===========================>..] - ETA: 1s - loss: 1.7906 - acc: 0.2140Epoch 00000: val_loss improved from inf to 1.71181, saving model to saved_models\\InceptionV3.hdf5\n",
      "27/27 [==============================] - 53s - loss: 1.7924 - acc: 0.2079 - val_loss: 1.7118 - val_acc: 0.3006\n",
      "Epoch 2/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.7027 - acc: 0.2587Epoch 00001: val_loss improved from 1.71181 to 1.58126, saving model to saved_models\\InceptionV3.hdf5\n",
      "27/27 [==============================] - 30s - loss: 1.7057 - acc: 0.2565 - val_loss: 1.5813 - val_acc: 0.3988\n",
      "Epoch 3/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.5683 - acc: 0.3690Epoch 00002: val_loss improved from 1.58126 to 1.35933, saving model to saved_models\\InceptionV3.hdf5\n",
      "27/27 [==============================] - 33s - loss: 1.5594 - acc: 0.3739 - val_loss: 1.3593 - val_acc: 0.5318\n",
      "Epoch 4/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.4137 - acc: 0.4654Epoch 00003: val_loss improved from 1.35933 to 1.27212, saving model to saved_models\\InceptionV3.hdf5\n",
      "27/27 [==============================] - 32s - loss: 1.4191 - acc: 0.4648 - val_loss: 1.2721 - val_acc: 0.5954\n",
      "Epoch 5/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.2951 - acc: 0.5424Epoch 00004: val_loss improved from 1.27212 to 1.19347, saving model to saved_models\\InceptionV3.hdf5\n",
      "27/27 [==============================] - 30s - loss: 1.2877 - acc: 0.5482 - val_loss: 1.1935 - val_acc: 0.6358\n",
      "Epoch 6/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.2395 - acc: 0.5678Epoch 00005: val_loss improved from 1.19347 to 1.08265, saving model to saved_models\\InceptionV3.hdf5\n",
      "27/27 [==============================] - 30s - loss: 1.2659 - acc: 0.5597 - val_loss: 1.0826 - val_acc: 0.6705\n",
      "Epoch 7/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.0900 - acc: 0.6291Epoch 00006: val_loss improved from 1.08265 to 1.03796, saving model to saved_models\\InceptionV3.hdf5\n",
      "27/27 [==============================] - 31s - loss: 1.0836 - acc: 0.6355 - val_loss: 1.0380 - val_acc: 0.7283\n",
      "Epoch 8/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.1237 - acc: 0.6251Epoch 00007: val_loss improved from 1.03796 to 0.97032, saving model to saved_models\\InceptionV3.hdf5\n",
      "27/27 [==============================] - 32s - loss: 1.1300 - acc: 0.6186 - val_loss: 0.9703 - val_acc: 0.7225\n",
      "Epoch 9/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.0158 - acc: 0.6932Epoch 00008: val_loss did not improve\n",
      "27/27 [==============================] - 31s - loss: 1.0091 - acc: 0.6935 - val_loss: 0.9887 - val_acc: 0.7457\n",
      "Epoch 10/10\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 1.0022 - acc: 0.6816Epoch 00009: val_loss improved from 0.97032 to 0.91730, saving model to saved_models\\InceptionV3.hdf5\n",
      "27/27 [==============================] - 34s - loss: 1.0123 - acc: 0.6767 - val_loss: 0.9173 - val_acc: 0.7457\n"
     ]
    }
   ],
   "source": [
    "# Train the models\n",
    "for i, model in enumerate(base_models):\n",
    "    print(\"\")\n",
    "    print(\"Training Model: %s\" % base_modelnames[i])\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(saved_models_dir, base_modelnames[i] + '.hdf5'), \n",
    "                           verbose=1, save_best_only=True)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # custom Layers \n",
    "    cus_layers = model.output\n",
    "    cus_layers = Flatten()(cus_layers)\n",
    "    cus_layers = Dense(1024, activation=\"relu\")(cus_layers)\n",
    "    cus_layers = Dropout(0.3)(cus_layers)\n",
    "    cus_layers = Dense(512, activation=\"relu\")(cus_layers)\n",
    "    cus_layers = Dropout(0.3)(cus_layers)\n",
    "    cus_layers = Dense(len(categories), activation=\"relu\")(cus_layers)\n",
    "    predictions = Dense(len(categories), activation=\"softmax\")(cus_layers)\n",
    "    \n",
    "    # create the final model \n",
    "    model_final = Model(inputs = model.input, outputs = predictions)\n",
    "\n",
    "    # compile the model \n",
    "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    model_final.fit_generator(train_generator,\n",
    "                             steps_per_epoch=training_count // 10, \n",
    "                              epochs=epochs,\n",
    "                             validation_data = validation_generator,\n",
    "                             validation_steps=validation_count // 10,\n",
    "                             callbacks=[checkpointer],\n",
    "                             verbose=1)\n",
    "    \n",
    "    model_final.load_weights(filepath=os.path.join(saved_models_dir, base_modelnames[i] + '.hdf5'))\n",
    "    \n",
    "    models.append(model_final)\n",
    "    modelnames.append(base_modelnames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Choosing the model\n",
    "\n",
    "Now that all of our models are trained, we can calculate their F1 scores on the test set. Important is that none of the models have seen data from the test set during the training phase, so it will be a good indication of how the model will perform in a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "# load the file locations and their labels 'y'\n",
    "test_files, y_test = preprocessor.load_dataset(test_dir)\n",
    "\n",
    "# load training, validation, and test matrices\n",
    "x_test = preprocessor.files_to_tensors(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donke\\Anaconda2\\envs\\cnn_image_classification\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random 0.164256772637\n",
      "benchmark 0.457219005606\n",
      "new cnn 0.521292327744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donke\\Anaconda2\\envs\\cnn_image_classification\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG19 0.098071931776\n",
      "ResNet50 0.050508039854\n",
      "InceptionV3 0.735185728948\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    f1score = f1_score_cal(model)\n",
    "    f1_scores.append(f1score)\n",
    "    print(modelnames[i], f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17eaae83e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(6)\n",
    "\n",
    "rects1 = ax.bar(index, f1_scores)\n",
    "\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Scores per model')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(modelnames)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Knowing that a higher F1 score is better, we can see that the **InceptionV3** model is outperforming all the other models with an F1 score of **0.74**. We'll take the InceptionV3 model and fine-tune as described in the [Fine-tuning the Model](Step 3 - Fine-tuning the Model.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn_image_classification]",
   "language": "python",
   "name": "conda-env-cnn_image_classification-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
